# -*- coding: utf-8 -*-
"""
Created on Fri Nov  6 22:52:49 2020

@author: Kapil
"""

import numpy as np
import pandas as pd
import torch
from torch import nn, optim
#from tqdm import tqdm
#import torch.optim as optim
from bertmodel import BertClassifier
#from torch.nn import functional as F
from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import Dataset, DataLoader
from collections import defaultdict
from sklearn.model_selection import train_test_split


#device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")


# Load the dataset into a pandas dataframe.
train_df = pd.read_csv('trac2_eng_train.csv')
#dev_df = pd.read_csv('trac2_eng_dev.csv')

# Report the number of sentences.
print('Number of training sentences: {:,}\n'.format(train_df.shape[0]))
# Display 10 random rows from the data.
#print(train_df.sample(10))

def prepare_sequence(seq, to_ix):
    idxs = [to_ix[w] for w in seq]
    return torch.tensor(idxs, dtype=torch.long)

# Get the lists of sentences and their labels.
sentences = train_df.Text.values
labels_ta = train_df.STA.values
labels_tb = train_df.STB.values



# Load the BERT tokenizer.
print('Loading BERT tokenizer...')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)


label2idx = {'NAG': 0, 'OAG': 1, 'CAG': 2}
idx2label = {0: 'NAG', 1: 'OAG', 2: 'CAG'}

gen2idx = {'NGEN': 0, 'GEN': 1}
idx2gen = {0: 'NGEN', 1: 'GEN'}

train_loss_list_a= []
train_loss_list_b= []
EPOCHS = 4


# Tokenize all of the sentences and map the tokens to thier word IDs.
input_ids = []
attention_masks = []
model = BertClassifier()
#model.eval()
# Loss function to use
loss_function = nn.NLLLoss()
# Optimizer to use during training
#optimizer = optim.SGD(model.parameters(), lr=0.1)
optimizer = optim.Adam(model.parameters(), lr=0.01)
count = 0;
texts = []


print("Training from scratch")
for epoch in range(1, EPOCHS + 1):  # normally you would NOT do 100 epochs, it is toy data
    print(f"Starting epoch {epoch}...")
    # For every sentence...
    for sent in sentences:
        # `encode` will:
        #   (1) Tokenize the sentence.
        #   (2) Prepend the `[CLS]` token to the start.
        #   (3) Append the `[SEP]` token to the end.
        #   (4) Map tokens to their IDs.
        encoded_sent = tokenizer.encode_plus(
                            text=sent,  # the sentence to be encoded
                            add_special_tokens=True,  # Add [CLS] and [SEP]
                            max_length = 10,  # maximum length of a sentence
                            return_token_type_ids=False,
                            pad_to_max_length=True,  # Add [PAD]s
                            return_attention_mask = True,  # Generate the attention mask
                            #return_tensors = 'pt',  # ask the function to return PyTorch tensors
                            truncation = True
                        )
        model.zero_grad()
        # Add the outputs to the lists
        texts.append((encoded_sent.get('text')))
        input_ids.append(encoded_sent.get('input_ids'))
        attention_masks.append(encoded_sent.get('attention_mask'))
        
        ip1 = input_ids
        att1 = attention_masks
        count = count + 1
        if(count>1):
            # Convert lists to tensors
            ip1 = torch.tensor(ip1)
            att1 = torch.tensor(att1)
            print('input_ids: ', input_ids)
            print('attention_masks: ', attention_masks)
            print('ip1: ', ip1)
            print('att1: ', att1)
            
        
            tag_scores_a, tag_scores_b, weights = model(ip1, att1)
            train_loss_a = loss_function(tag_scores_a, targets)
            train_loss_b = loss_function(tag_scores_b, targets)
            
            train_loss_a.backward()
            train_loss_b.backward()
            
            optimizer.step()
train_loss_list_a.append(train_loss_a)
print("Training Loss for {} epoch: {}".format(epoch, train_loss_a))   
train_loss_list_b.append(train_loss_b)
print("Training Loss for {} epoch: {}".format(epoch, train_loss_b))  
    

# Convert lists to tensors
input_ids = torch.tensor(input_ids)
attention_masks = torch.tensor(attention_masks)


# Print sentence 0, now as a list of IDs.
print('Original: ', sentences[3])
print('Token IDs:', input_ids[3])
print('Attention Mask:', attention_masks[3])
print('Sub task A:', labels_ta[3])
print('Sub task B:', labels_tb[3])

op1, op2, weights = model(input_ids, attention_masks)  





# RANDOM_SEED = 42
# np.random.seed(RANDOM_SEED)
# torch.manual_seed(RANDOM_SEED)

# df_train, df_test = train_test_split(
#   train_df,
#   test_size=0.1,
#   random_state=RANDOM_SEED
# )
# df_val, df_test = train_test_split(
#   df_test,
#   test_size=0.5,
#   random_state=RANDOM_SEED
# )


# class GPReviewDataset(Dataset):
#   def __init__(self, reviews, targets, tokenizer, max_len):
#     self.reviews = reviews
#     self.targets = targets
#     self.tokenizer = tokenizer
#     self.max_len = max_len
#   def __len__(self):
#     return len(self.reviews)
#   def __getitem__(self, item):
#     review = str(self.reviews[item])
#     target = self.targets[item]
#     encoding = self.tokenizer.encode_plus(
#       review,
#       add_special_tokens=True,
#       max_length=self.max_len,
#       return_token_type_ids=False,
#       pad_to_max_length=True,
#       return_attention_mask=True,
#       return_tensors='pt',
#     )
#     return {
#       'review_text': review,
#       'input_ids': encoding['input_ids'].flatten(),
#       'attention_mask': encoding['attention_mask'].flatten(),
#       'targets': torch.tensor(target, dtype=torch.long)
#     }


# def create_data_loader(df, tokenizer, max_len, batch_size):
#   ds = GPReviewDataset(
#     reviews=df.Text.to_numpy(),
#     targets=df.STA.to_numpy(),
#     tokenizer=tokenizer,
#     max_len=max_len
#   )
#   return DataLoader(
#     ds,
#     batch_size=batch_size,
#     num_workers=4
#   )

# BATCH_SIZE = 16
# MAX_LEN = 200
# train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
# val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)
# test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)


# #EPOCHS = 10
# optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
# total_steps = len(train_data_loader) * EPOCHS
# scheduler = get_linear_schedule_with_warmup(
#   optimizer,
#   num_warmup_steps=0,
#   num_training_steps=total_steps
# )
# loss_fn = nn.CrossEntropyLoss()


# def train_epoch(
#   model,
#   data_loader,
#   loss_fn,
#   optimizer,
#   scheduler,
#   n_examples
# ):
#   model = model.train()
#   losses = []
#   correct_predictions = 0
#   for d in data_loader:
#     input_ids = d["input_ids"]
#     attention_mask = d["attention_mask"]
#     targets = d["targets"]
#     outputs = model(
#       input_ids=input_ids,
#       attention_mask=attention_mask
#     )
#     _, preds = torch.max(outputs, dim=1)
#     loss = loss_fn(outputs, targets)
#     correct_predictions += torch.sum(preds == targets)
#     losses.append(loss.item())
#     loss.backward()
#     nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
#     optimizer.step()
#     scheduler.step()
#     optimizer.zero_grad()
#   return correct_predictions.double() / n_examples, np.mean(losses)


# def eval_model(model, data_loader, loss_fn, n_examples):
#   model = model.eval()
#   losses = []
#   correct_predictions = 0
#   with torch.no_grad():
#     for d in data_loader:
#       input_ids = d["input_ids"]
#       attention_mask = d["attention_mask"]
#       targets = d["targets"]
#       outputs = model(
#         input_ids=input_ids,
#         attention_mask=attention_mask
#       )
#       _, preds = torch.max(outputs, dim=1)
#       loss = loss_fn(outputs, targets)
#       correct_predictions += torch.sum(preds == targets)
#       losses.append(loss.item())
#   return correct_predictions.double() / n_examples, np.mean(losses)



# history = defaultdict(list)
# best_accuracy = 0
# for epoch in range(EPOCHS):
#   print(f'Epoch {epoch + 1}/{EPOCHS}')
#   print('-' * 10)
#   train_acc, train_loss = train_epoch(
#     model,
#     train_data_loader,
#     loss_fn,
#     optimizer,
#     scheduler,
#     len(df_train)
#   )
#   print(f'Train loss {train_loss} accuracy {train_acc}')
#   val_acc, val_loss = eval_model(
#     model,
#     val_data_loader,
#     loss_fn,
#     len(df_val)
#   )
#   print(f'Val   loss {val_loss} accuracy {val_acc}')
#   print()
#   history['train_acc'].append(train_acc)
#   history['train_loss'].append(train_loss)
#   history['val_acc'].append(val_acc)
#   history['val_loss'].append(val_loss)
#   if val_acc > best_accuracy:
#     torch.save(model.state_dict(), 'best_model_state.bin')
#     best_accuracy = val_acc

